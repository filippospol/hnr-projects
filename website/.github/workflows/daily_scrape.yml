name: Daily NBA Data Scrape

# Schedule script:
on:
  schedule:
    # Every day at 08:00:
    - cron: '0 8 * * *' 
  workflow_dispatch: # button for manual option

jobs:
  scrape-data:
    runs-on: ubuntu-latest # Cloud computer type

    steps:
      # 1. Open your repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2. Install R on the cloud computer
      - name: Setup R
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: 'release'

      # 3. Install necessary background software for R packages (like rvest and httr)
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev

      # 4. Install pacman and run your script
      - name: Run scraper script
        run: |
          Rscript -e 'install.packages("pacman", repos="https://cloud.r-project.org")'
          Rscript hnrdb_full.R

      # 5. Save the newly generated CSV files back to GitHub
      - name: Commit and push new data
        run: |
          git config --local user.name "GitHub Actions Bot"
          git config --local user.email "actions@github.com"
          git add *.csv
          git commit -m "Auto-update daily NBA stats" || echo "No changes to commit"
          git push origin main
